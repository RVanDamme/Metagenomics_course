{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Metagenomics","text":""},{"location":"#introduction","title":"Introduction","text":"<p>https://usegalaxy.eu/join-training/heh-mag2024</p> <ol> <li>Introduction to Galaxy here </li> <li>Introduction to Metagenomics and environmental studies Slides 1 Slides 2</li> <li>The global presentation</li> <li>Muffin A nextflow Genomic Workflow Slides</li> </ol>"},{"location":"#additional-introductions","title":"Additional Introductions:","text":"<ul> <li>Link to all Galaxy Metagenomics courses</li> <li>Analyses of metagenomics data - The global picture 2,5-3h by Galaxy</li> <li>Ancient DNA and metagenomics Slides</li> </ul>"},{"location":"#shotgun-metagenomics-assembly-free","title":"Shotgun metagenomics (assembly free)","text":"<p>Slides</p> <p>Assembly-free method, Practical 4h</p>"},{"location":"#galaxy-alternative","title":"Galaxy alternative","text":"<p>Identification of the micro-organisms in a beer using Nanopore sequencing 1-1,5h</p>"},{"location":"#shotgun-metagenomics-with-assembly","title":"Shotgun metagenomics (with assembly)","text":"<p>Assembly and binning method, Practical 4h</p>"},{"location":"#galaxy-alternative_1","title":"Galaxy alternative","text":"<p>Assembly of metagenomic sequencing data 2-3h</p>"},{"location":"#amplicon-sequencing","title":"Amplicon sequencing","text":"<p>Slides</p> <p>16S Microbial Analysis with mothur (short) 2-3h</p>"},{"location":"#alternatives","title":"alternatives","text":"<p>Galaxy: 16S Microbial analysis with Nanopore data 2-3h</p> <p>Local: DADA2</p>"},{"location":"#metatranscriptomics","title":"Metatranscriptomics","text":"<p>Metatranscriptomics analysis using microbiome RNA-seq data (short) 3-4h</p>"},{"location":"#grade-breakdown","title":"Grade breakdown","text":"<ul> <li>40% Active participation</li> <li>20% Project report</li> <li>40% Pre-Project presentation and discussion</li> </ul>"},{"location":"#project","title":"Project","text":""},{"location":"#instructions","title":"instructions:","text":"<p>Find a tool, pipeline, or analysis method of metagenomics that has not been seen during the course and test and evaluate it. You must explain how it works, how you found it, and its use in metagenomics studies. You can compare it to a similar tool/method seen during the course.</p> <p>You are tasked to have a report of at least two pages (not counting graphics, pictures, and tables). Additional points will be provided if your work is easily reproducible (scripts, command list, and workflows provided in addition to the report).</p>"},{"location":"#project-grade-breakdown","title":"Project Grade breakdown","text":"<ul> <li>10% originality</li> <li>20% Reasoning and Explanation of what it should do</li> <li>30% execution and testing</li> <li>20% analysis of the results</li> <li>20% comparison to existing methods</li> </ul>"},{"location":"#presentation-plan","title":"Presentation Plan","text":"<p>You will present your individual project and a summary of the course.</p> <p>15 minutes of group presentation where you summarise the principles and methods of metagenomics.</p> <p>15 minutes of individual presentation where you expose your work:</p> <ul> <li> <p>Methods selected and Why</p> </li> <li> <p>Results</p> </li> <li> <p>Discussion (comparison with a method seen during the course)</p> </li> </ul>"},{"location":"#bibliography-and-ressources","title":"Bibliography and ressources","text":"<p>Core Papers: </p> <ul> <li> <p>https://www.microbiologyresearch.org/content/journal/mgen/10.1099/mgen.0.000409</p> </li> <li> <p>https://www.nature.com/articles/nbt.3935/figures/1</p> </li> <li> <p>https://www.sciencedirect.com/topics/biochemistry-genetics-and-molecular-biology/metagenomics</p> </li> </ul> <p>Additional read: </p> <ul> <li> <p>https://www.nature.com/articles/nbt.3935</p> </li> <li> <p>https://www.nature.com/articles/srep01968</p> </li> <li> <p>https://www.frontiersin.org/articles/10.3389/fmicb.2021.613791/full</p> </li> <li> <p>https://academic.oup.com/bib/article/22/6/bbab330/6358409</p> </li> <li> <p>https://academic.oup.com/bib/article/21/2/584/5363831</p> </li> <li> <p>https://www.nature.com/articles/s42003-021-02510-6</p> </li> <li> <p>https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-020-03667-3</p> </li> </ul> <p>Tools:</p> <ul> <li> <p>MUFFIN: https://github.com/RVanDamme/MUFFIN &amp; https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008716</p> </li> <li> <p>metaWRAP: https://github.com/bxlab/metaWRAP &amp; https://microbiomejournal.biomedcentral.com/articles/10.1186/s40168-018-0541-1</p> </li> <li> <p>Flye: https://github.com/fenderglass/Flye &amp; https://www.nature.com/articles/s41592-020-00971-x</p> </li> <li> <p>Kraken2: https://github.com/DerrickWood/kraken2 &amp; https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1891-0</p> </li> </ul>"},{"location":"#galaxy","title":"Galaxy","text":""},{"location":"#galaxy-set-up","title":"Galaxy set up","text":"<ol> <li>Register to usegalaxy.eu.</li> </ol> <ol> <li> <p>Click the verification link you will receive by email.</p> </li> <li> <p>Go to this https://usegalaxy.eu/join-training//heh-mag2024 to join our training session.  It should say you successfully registered in heh-meta-omic</p> </li> </ol> <p></p>"},{"location":"#galaxy-introduction","title":"Galaxy introduction","text":"<p>Slides here</p> <p>We will follow this tutorial</p>"},{"location":"#bioconda-and-conda-installation","title":"Bioconda and Conda installation","text":"<p>We will follow the installation recommended by bioconda directly</p>"},{"location":"#installation-of-bioconda","title":"Installation of bioconda","text":"<ol> <li>Install conda</li> </ol> <p>Bioconda requires the conda package manager to be installed. If you have an Anaconda Python installation, you already have it. Otherwise, the best way to install it is with the Miniconda package. The Python 3 version is recommended.</p> <p>On MacOS, run:</p> <pre><code>curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-x86_64.sh\nsh Miniconda3-latest-MacOSX-x86_64.sh\n</code></pre> <p>On Linux, run:</p> <pre><code>curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\nsh Miniconda3-latest-Linux-x86_64.sh\n</code></pre> <p>Follow the instructions in the installer. If you encounter problems, refer to the Miniconda documentation.</p> <ol> <li>Set up channels</li> </ol> <p>After installing conda, you will need to add the bioconda channel as well as the other channels bioconda depends on. It is important to add them in this order to set the priority correctly (that is, conda-forge is the highest priority).</p> <p>The conda-forge channel contains many general-purpose packages not already found in the default channel.</p> <pre><code>conda config --add channels defaults\nconda config --add channels bioconda\nconda config --add channels conda-forge\n</code></pre> <ol> <li>Install packages</li> </ol> <p>Browse the packages to see what\u2019s available.</p> <p>Bioconda is now enabled, so any packages on the bioconda channel can be installed into the current conda environment:</p> <pre><code>conda install bwa\n</code></pre> <p>Or a new environment can be created:</p> <pre><code>conda create -n aligners bwa bowtie hisat star\n</code></pre> <p>!!!! if you are on Ubuntu in WSL, you might get an HTTP error. To solve it, a simple reboot of your computer is enough.!!!!</p>"},{"location":"#conda-tutorial-guides-and-cheatsheet","title":"Conda Tutorial, guides, and CheatSheet","text":"<p>Here is a collection of materials to train and improve your conda use. All have their strength and weaknesses, the cheatsheet is your best friend when working.</p> <ul> <li>NBIS tutorial</li> <li>Guide by Matthew Sarmiento</li> <li>Guide by Whitebox</li> <li>CheatSheet</li> </ul>"},{"location":"usage/assembly_binning_based/","title":"Metagenome assembly and binning","text":"<p>Before anything RUN THIS:</p> <pre><code>conda create -n Meta_assembly -c bioconda fastqc sickle-trim megahit bowtie2 samtools metabat2=2.15 checkm-genome prokka\nconda create -n sour -c bioconda sourmash=4.5.0\n</code></pre> <p>In this tutorial you'll learn how to inspect assemble metagenomic data and retrieve draft genomes from assembled metagenomes</p> <p>We'll use a mock community of 20 bacteria sequenced using the Illumina HiSeq. In reality the data were simulated using InSilicoSeq.</p> <p>The 20 bacteria in the dataset were selected from the Tara Ocean study that recovered 957 distinct Metagenome-assembled-genomes (or MAGs) that were previsouly unknown! (full list on figshare )</p>"},{"location":"usage/assembly_binning_based/#getting-the-data","title":"Getting the Data","text":"<pre><code>mkdir -p ~/data\ncd ~/data\ncurl -O -J -L https://osf.io/th9z6/download\ncurl -O -J -L https://osf.io/k6vme/download\nchmod -w tara_reads_R*\n</code></pre>"},{"location":"usage/assembly_binning_based/#quality-control","title":"Quality Control","text":"<p>we'll use FastQC to check the quality of our data, as well as sickle for trimming the bad quality part of the reads. If you need a refresher on how and why to check the quality of sequence data, please check the Quality Control and Trimming tutorial</p> <pre><code>mkdir -p ~/results\ncd ~/results\nln -s ~/data/tara_reads_* .\nfastqc tara_reads_*.fastq.gz\n</code></pre> <p>!!! question     What is the average read length? The average quality?</p> <p>!!! question     Compared to single genome sequencing, what graphs differ?</p> <p>Now we'll trim the reads using sickle</p> <pre><code>sickle pe -f tara_reads_R1.fastq.gz -r tara_reads_R2.fastq.gz -t sanger \\\n    -o tara_trimmed_R1.fastq -p tara_trimmed_R2.fastq -s /dev/null\n</code></pre> <p>!!! question     How many reads were trimmed?</p>"},{"location":"usage/assembly_binning_based/#assembly","title":"Assembly","text":"<p>Megahit will be used for the assembly.</p> <pre><code>megahit -1 tara_trimmed_R1.fastq -2 tara_trimmed_R2.fastq -o tara_assembly\n</code></pre> <p>the resulting assembly can be found under <code>tara_assembly/final.contigs.fa</code>.</p> <p>!!! question     How many contigs does this assembly contain?</p>"},{"location":"usage/assembly_binning_based/#binning","title":"Binning","text":"<p>First we need to map the reads back against the assembly to get coverage information</p> <pre><code>ln -s tara_assembly/final.contigs.fa .\nbowtie2-build final.contigs.fa final.contigs\nbowtie2 -x final.contigs -1 tara_reads_R1.fastq.gz -2 tara_reads_R2.fastq.gz | \\\n    samtools view -bS -o tara_to_sort.bam\nsamtools sort tara_to_sort.bam -o tara.bam\nsamtools index tara.bam\n</code></pre> <p>then we run metabat</p> <pre><code>runMetaBat.sh -m 1500 final.contigs.fa tara.bam\nmv final.contigs.fa.metabat-bins1500 metabat\n</code></pre> <p>!!! question     How many bins did we obtain?</p>"},{"location":"usage/assembly_binning_based/#checking-the-quality-of-the-bins","title":"Checking the quality of the bins","text":"<p>The first time you run <code>checkm</code> you have to create the database</p> <pre><code>mkdir   db/\nwget --no-check-certificate https://data.ace.uq.edu.au/public/CheckM_databases/checkm_data_2015_01_16.tar.gz\nmv  checkm_data_2015_01_16.tar.gz db/\ncd db/\ntar -xzvf checkm_data_2015_01_16.tar.gz\ncd ..\ncheckm data setRoot db/\n</code></pre> <pre><code>checkm lineage_wf -x fa metabat checkm/\ncheckm qa  checkm/lineage.ms checkm &gt; checkm.txt\n</code></pre> <p>!!! question     Which bins should we keep for downstream analysis?</p>"},{"location":"usage/assembly_binning_based/#functional-annotation","title":"Functional annotation","text":"<p>Now we should have a collection of MAGs that we can further analyze. The first step is to predict genes as right now we only have raw genomic sequences. We will use one of my all-time-favorites : prokka.</p> <p>This tool does gene prediction as well as some decent and useful annotations, and is actually quite easy to run!</p> <p>Prokka produces a number of output files that all kinds of represent similar things. Mostly variants of FASTA-files, one with the genome again, one with the predicted proteins, one with the genes of the predicted proteins. Also, it renames all the sequences with nicer IDs! Additionally, a very useful file generated is a GFF-file, which gives more information about the annotations than just the names you can see in the FASTA-files.</p> <p>The annotations of prokka are good but not very complete for environmental bacteria. Let\u2019s run an other tool I like a lot, eggNOGmapper. This is a bit heavier in computation.</p> <pre><code>Install eggNOG-mapper run it on at least one MAG (don\u2019t be greedy, it is not fast!) OPTIONAL : undestand the output of it \u2026\n</code></pre>"},{"location":"usage/assembly_binning_based/#taxonomic-annotation","title":"Taxonomic annotation","text":"<p>We now know more about the genes your MAG contains, however we do not really know who we have?! checkm might have given us an indication but it is only approximative.</p> <p>Taxonomic classification for full genomes is not always easy for MAGs, often the 16S gene is missing as it assembles badly, and which other genes to use to for taxonomy is not always evident. Typically marker genes, min-hashes or k-mer databases are used as reference. It is often problematic for environmental data as the databases are not biased into our direction! We will use a min-hash database I compiled specifically for this (based on other tools and the full-datasets) using a tool called sourmash</p> <pre><code>Install sourmash in your computer!\n\n```bash\nconda deactivate\nconda activate sour\nsourmash compute -k 31 --scaled 10000 metabat/bin.* \n```\n\nto compute signatures Use the lca clasify function of sourmash with the database you can find here\n\n```bash\nwget https://osf.io/download/p9ezm -O gtdb-rs207.genomic-reps.dna.k31.lca.json.gz\n\nsourmash lca classify --query bin.1.fa.sig --db gtdb-rs207.genomic-reps.dna.k31.lca.json.gz\n```\n</code></pre> <p>This database was made by the representative genomes of the gtdb database</p> <p>Links to an external site.. The GTDB database is also used by the tool gtdbtk, it uses marker genes and loads of data. It is a bit heavy, and tricky to run/install but much more sensitive.</p> <pre><code>Optional: install and run gtdbtk\n</code></pre>"},{"location":"usage/assembly_binning_based/#further-reading","title":"Further reading","text":"<ul> <li>Recovery of nearly 8,000 metagenome-assembled genomes substantially expands the tree of life</li> <li>The reconstruction of 2,631 draft metagenome-assembled genomes from the global oceans</li> </ul>"},{"location":"usage/assembly_free/","title":"Whole Metagenome Sequencing","text":"<p>Before anything RUN THIS:</p> <pre><code>conda create -n Meta_free -c bioconda fastqc kraken2\n</code></pre>"},{"location":"usage/assembly_free/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Introduction<ul> <li>The Pig Microbiome</li> <li>Whole Metagenome Sequencing</li> </ul> </li> <li>Softwares Required for this Tutorial</li> <li>Getting the Data and Checking their Quality</li> <li>Taxonomic Classification</li> <li>Visualization</li> </ul>"},{"location":"usage/assembly_free/#introduction","title":"Introduction","text":""},{"location":"usage/assembly_free/#microbiome-used","title":"Microbiome used","text":"<p>In this tutorial we will compare samples from the Pig Gut Microbiome to samples from the Human Gut Microbiome. Below you'll find a brief description of the two projects:</p> <p>The Pig Microbiome:</p> <p>Pig is a main species for livestock and biomedicine. The pig genome sequence was recently reported. To boost research, we established a catalogue of the genes of the gut microbiome based on faecal samples of 287 pigs from France, Denmark and China. More than 7.6 million non-redundant genes representing 719 metagenomic species were identified by deep metagenome sequencing, highlighting more similarities with the human than with the mouse catalogue. The pig and human catalogues share only 12.6 and 9.3 % of their genes, respectively, but 70 and 95% of their functional pathways. The pig gut microbiota is influenced by gender, age and breed. Analysis of the prevalence of antibiotics resistance genes (ARGs) reflected antibiotics supplementation in each farm system, and revealed that non-antibiotics-fed animals still harbour ARGs. The pig catalogue creates a resource for whole metagenomics-based studies, highly valuable for research in biomedicine and for sustainable knowledge-based pig farming</p> <p>The Human Microbiome:</p> <p>We are facing a global metabolic health crisis provoked by an obesity epidemic. Here we report the human gut microbial composition in a population sample of 123 non-obese and 169 obese Danish individuals. We find two groups of individuals that differ by the number of gut microbial genes and thus gut bacterial richness. They harbour known and previously unknown bacterial species at different proportions; individuals with a low bacterial richness (23% of the population) are characterized by more marked overall adiposity, insulin resistance and dyslipidaemia and a more pronounced inflammatory phenotype when compared with high bacterial richness individuals. The obese individuals among the former also gain more weight over time. Only a few bacterial species are sufficient to distinguish between individuals with high and low bacterial richness, and even between lean and obese. Our classifications based on variation in the gut microbiome identify subsets of individuals in the general white adult population who may be at increased risk of progressing to adiposity-associated co-morbidities</p>"},{"location":"usage/assembly_free/#whole-metagenome-sequencing_1","title":"Whole Metagenome Sequencing","text":"<p>Whole Metagenome sequencing (WMS), or shotgun metagenome sequencing, is a relatively new and powerful sequencing approach that provides insight into community biodiversity and function. On the contrary of Metabarcoding, where only a specific region of the bacterial community (the 16s rRNA) is sequenced, WMS aims at sequencing all the genomic material present in the environment.</p> <p>The choice of shotgun or 16S approaches is usually dictated by the nature of the studies being conducted. For instance, 16S is well suited for analysis of large number of samples, i.e., multiple patients, longitudinal studies, etc. but offers limited taxonomical and functional resolution. WMS is generally more expensive but offers increased resolution, and allows the discovery of viruses as well as other mobile genetic elements.</p>"},{"location":"usage/assembly_free/#softwares-required-for-this-tutorial","title":"Softwares Required for this Tutorial","text":"<ul> <li>FastQC</li> <li>Kraken</li> <li>R</li> <li>Pavian</li> </ul>"},{"location":"usage/assembly_free/#prepare-and-organise-your-working-directory","title":"Prepare and organise your working directory","text":"<p>You will first login to your virtual machine using the IP provided by the teachers. All the exercise will be performed on your VM in the cloud.</p> <p>!!! note     When you login with the ssh command, please add the option -X at the end of it to be able to use graphical interface</p> <pre><code>mkdir ~/wms\ncd ~/wms\nmkdir data\nmkdir results\nmkdir scripts\n</code></pre>"},{"location":"usage/assembly_free/#getting-the-data-and-checking-their-quality","title":"Getting the Data and Checking their Quality","text":"<p>As the data were very big, we have prepared performed a downsampling on all 6 datasets (3 pigs and 3 humans). We will first download and unpack the data.</p> <pre><code>cd ~/wms/data\ncurl -O -J -L https://osf.io/h9x6e/download\ntar xvf subset_wms.tar.gz\ncd sub_100000\n</code></pre> <p>We'll use FastQC to check the quality of our data. FastQC should be already installed on your VM, so you need to type</p> <pre><code>fastqc *.fastq\n</code></pre> <p>If the quality appears to be good, it's because it was probably the cleaned reads that were deposited into SRA. We can directly move to the classification step.</p>"},{"location":"usage/assembly_free/#taxonomic-classification","title":"Taxonomic Classification","text":"<p>Kraken is a system for assigning taxonomic labels to short DNA sequences (i.e. reads) Kraken aims to achieve high sensitivity and high speed by utilizing exact alignments of k-mers and a novel classification algorithm (sic).</p> <p>In short, kraken uses a new approach with exact k-mer matching to assign taxonomy to short reads. It is extremely fast compared to traditional approaches (i.e. BLAST).</p> <p>By default, the authors of kraken built their database based on RefSeq Bacteria, Archaea and Viruses. We'll use it for the purpose of this tutorial. We will download a shrunk database (minikraken) provided by Kraken developers that is only 4GB.</p> <pre><code># First we create a databases directory in our home\ncd /mnt\nsudo mkdir databases\ncd databases\n# Then we download the minikraken database\nsudo wget https://ccb.jhu.edu/software/kraken/dl/minikraken_20171019_4GB.tgz\nsudo tar xzf minikraken_20171019_4GB.tgz\nKRAKEN_DB=/mnt/databases/minikraken_20171013_4GB\ncd\n</code></pre> <p>Now run kraken on the reads</p> <pre><code># In the data/ directory\ncd ~/wms/data/sub_100000\nfor i in *_1.fastq\ndo\n    prefix=$(basename $i _1.fastq)\n    # print which sample is being processed\n    echo $prefix\n    kraken --db $KRAKEN_DB --threads 2 --fastq-input \\\n        ${prefix}_1.fastq ${prefix}_2.fastq &gt; /home/student/wms/results/${prefix}.tab\n    kraken-report --db $KRAKEN_DB \\\n        /home/student/wms/results/${prefix}.tab &gt; /home/student/wms/results/${prefix}_tax.txt\ndone\n</code></pre> <p>which produces a tab-delimited file with an assigned TaxID for each read.</p> <p>Kraken includes a script called <code>kraken-report</code> to transform this file into a \"tree\" view with the percentage of reads assigned to each taxa. We've run this script at each step in the loop. Take a look at the <code>_tax.txt</code> files!</p>"},{"location":"usage/assembly_free/#visualization-with-pavian","title":"Visualization with Pavian","text":"<p>Pavian is a web application for exploring metagenomics classification results.</p> <p>Install and run Pavian from R:</p> <pre><code>options(repos = c(CRAN = \"http://cran.rstudio.com\"))\nif (!require(remotes)) { install.packages(\"remotes\") }\nremotes::install_github(\"fbreitwieser/pavian\")\npavian::runApp(port=5000)\n</code></pre> <p>Then you will explore and compare the results produced by Kraken.</p> <p>Try to use Pavian to look at this data a bit!!</p>"},{"location":"usage/assembly_free/#functional-classification","title":"Functional Classification","text":"<p>You can use the humann2 pipeline for functional classification</p>"},{"location":"usage/metabarcoding/","title":"Metabarcoding","text":"<p>This tutorial is aimed at being a walkthrough of the DADA2 pipeline. It uses the data of the now famous MiSeq SOP by the Mothur authors but analyses the data using DADA2.</p> <p>DADA2 is a relatively new method to analyse amplicon data which uses exact variants instead of OTUs.    </p> <p>The advantages of the DADA2 method is described in the paper</p>"},{"location":"usage/metabarcoding/#before-starting","title":"Before Starting","text":"<p>There are two ways to follow this tutorial: you can copy and paste all the codes blocks below in R directly, or you can download this document in the Rmarkdown format and execute the cells.</p> <ul> <li>Link to the document in Rmarkdown</li> </ul>"},{"location":"usage/metabarcoding/#install-and-load-packages","title":"Install and Load Packages","text":"<p>First install DADA2 and other necessary packages (line by line)</p> <p>```r {r install_pkg} if (!require(\"BiocManager\", quietly = TRUE))     install.packages(\"BiocManager\")</p>"},{"location":"usage/metabarcoding/#biocmanagerinstallcdada2phyloseqdecipher","title":"BiocManager::install(c(\"dada2\",\"phyloseq\",\"DECIPHER\"))","text":""},{"location":"usage/metabarcoding/#installpackagesggplot2","title":"install.packages('ggplot2')","text":""},{"location":"usage/metabarcoding/#installpackagesphangorn","title":"install.packages('phangorn')","text":""},{"location":"usage/metabarcoding/#biocmanagerinstallcgenomicranges","title":"BiocManager::install(c(\"GenomicRanges\"))","text":""},{"location":"usage/metabarcoding/#biocmanagerinstallcdelayedarray","title":"BiocManager::install(c(\"DelayedArray\"))","text":"<pre><code>\nNow load the packages and verify you have the correct DADA2 version\n\n```r\nlibrary(dada2)\nlibrary(ggplot2)\nlibrary(phyloseq)\nlibrary(phangorn)\nlibrary(DECIPHER)\npackageVersion('dada2')\n</code></pre>"},{"location":"usage/metabarcoding/#download-the-data","title":"Download the Data","text":"<p>You will also need to download the data, as well as the SILVA database</p> <p>!!! warning     If you are following the tutorial on the website, the following block of commands has to be executed outside of R.     If you run this tutorial with the R notebook, you can simply execute the cell block</p> <pre><code>wget http://www.mothur.org/w/images/d/d6/MiSeqSOPData.zip\nunzip MiSeqSOPData.zip\nrm -r __MACOSX/\ncd MiSeq_SOP\nwget https://zenodo.org/record/824551/files/silva_nr_v128_train_set.fa.gz\nwget https://zenodo.org/record/824551/files/silva_species_assignment_v128.fa.gz\ncd ..\n</code></pre> <p>Back in R, check that you have downloaded the data</p> <pre><code>path &lt;- 'MiSeq_SOP'\nlist.files(path)\n</code></pre>"},{"location":"usage/metabarcoding/#filtering-and-trimming","title":"Filtering and Trimming","text":"<p>First we create two lists with the sorted name of the reads: one for the forward reads, one for the reverse reads</p> <pre><code>raw_forward &lt;- sort(list.files(path, pattern=\"_R1_001.fastq\",\n                               full.names=TRUE))\n\nraw_reverse &lt;- sort(list.files(path, pattern=\"_R2_001.fastq\",\n                               full.names=TRUE))\n\n# we also need the sample names\nsample_names &lt;- sapply(strsplit(basename(raw_forward), \"_\"),\n                       `[`,  # extracts the first element of a subset\n                       1)\n</code></pre> <p>then we visualise the quality of our reads</p> <pre><code>plotQualityProfile(raw_forward[1:2])\nplotQualityProfile(raw_reverse[1:2])\n</code></pre> <p>!!! question     What do you think of the read quality?</p> <p>The forward reads are good quality (although dropping a bit at the end as usual) while the reverse are way worse.</p> <p>Based on these profiles, we will truncate the forward reads at position 240 and the reverse reads at position 160 where the quality distribution crashes.</p> <p>!!! note     in this tutorial we perform the trimming using DADA2's own functions.     If you wish to do it outside of DADA2, you can refer to the Quality Control tutorial</p> <p>Dada2 requires us to define the name of our output files</p> <pre><code># place filtered files in filtered/ subdirectory\nfiltered_path &lt;- file.path(path, \"filtered\")\n\nfiltered_forward &lt;- file.path(filtered_path,\n                              paste0(sample_names, \"_R1_trimmed.fastq.gz\"))\n\nfiltered_reverse &lt;- file.path(filtered_path,\n                              paste0(sample_names, \"_R2_trimmed.fastq.gz\"))\n</code></pre> <p>We\u2019ll use standard filtering parameters: <code>maxN=0</code> (DADA22 requires no Ns), <code>truncQ=2</code>, <code>rm.phix=TRUE</code> and <code>maxEE=2</code>. The maxEE parameter sets the maximum number of \u201cexpected errors\u201d allowed in a read, which according to the USEARCH authors is a better filter than simply averaging quality scores.</p> <pre><code>out &lt;- filterAndTrim(raw_forward, filtered_forward, raw_reverse,\n                     filtered_reverse, truncLen=c(240,160), maxN=0,\n                     maxEE=c(2,2), truncQ=2, rm.phix=TRUE, compress=TRUE,\n                     multithread=TRUE)\nhead(out)\n</code></pre>"},{"location":"usage/metabarcoding/#learn-the-error-rates","title":"Learn the Error Rates","text":"<p>The DADA2 algorithm depends on a parametric error model and every amplicon dataset has a slightly different error rate. The <code>learnErrors</code> of Dada2 learns the error model from the data and will help DADA2 to fits its method to your data</p> <pre><code>errors_forward &lt;- learnErrors(filtered_forward, multithread=TRUE)\nerrors_reverse &lt;- learnErrors(filtered_reverse, multithread=TRUE)\n</code></pre> <p>then we visualise the estimated error rates</p> <pre><code>plotErrors(errors_forward, nominalQ=TRUE) +\n    theme_minimal()\n</code></pre> <p>!!! question     Do you think the error model fits your data correctly?</p>"},{"location":"usage/metabarcoding/#dereplication","title":"Dereplication","text":"<p>From the Dada2 documentation:</p> <p>Dereplication combines all identical sequencing reads into into \u201cunique sequences\u201d with a corresponding \u201cabundance\u201d: the number of reads with that unique sequence. Dereplication substantially reduces computation time by eliminating redundant comparisons.</p> <pre><code>derep_forward &lt;- derepFastq(filtered_forward, verbose=TRUE)\nderep_reverse &lt;- derepFastq(filtered_reverse, verbose=TRUE)\n# name the derep-class objects by the sample names\nnames(derep_forward) &lt;- sample_names\nnames(derep_reverse) &lt;- sample_names\n</code></pre>"},{"location":"usage/metabarcoding/#sample-inference","title":"Sample inference","text":"<p>We are now ready to apply the core sequence-variant inference algorithm to the dereplicated data.</p> <pre><code>dada_forward &lt;- dada(derep_forward, err=errors_forward, multithread=TRUE)\ndada_reverse &lt;- dada(derep_reverse, err=errors_reverse, multithread=TRUE)\n\n# inspect the dada-class object\ndada_forward[[1]]\n</code></pre> <p>The DADA2 algorithm inferred 128 real sequence variants from the 1979 unique sequences in the first sample.</p>"},{"location":"usage/metabarcoding/#merge-paired-end-reads","title":"Merge Paired-end Reads","text":"<p>Now that the reads are trimmed, dereplicated and error-corrected we can merge them together</p> <pre><code>merged_reads &lt;- mergePairs(dada_forward, derep_forward, dada_reverse,\n                           derep_reverse, verbose=TRUE)\n\n# inspect the merger data.frame from the first sample\nhead(merged_reads[[1]])\n</code></pre>"},{"location":"usage/metabarcoding/#construct-sequence-table","title":"Construct Sequence Table","text":"<p>We can now construct a sequence table of our mouse samples, a higher-resolution version of the OTU table produced by traditional methods.</p> <pre><code>seq_table &lt;- makeSequenceTable(merged_reads)\ndim(seq_table)\n\n# inspect distribution of sequence lengths\ntable(nchar(getSequences(seq_table)))\n</code></pre>"},{"location":"usage/metabarcoding/#remove-chimeras","title":"Remove Chimeras","text":"<p>The <code>dada</code> method used earlier removes substitutions and indel errors but chimeras remain. We remove the chimeras with</p> <pre><code>seq_table_nochim &lt;- removeBimeraDenovo(seq_table, method='consensus',\n                                       multithread=TRUE, verbose=TRUE)\ndim(seq_table_nochim)\n\n# which percentage of our reads did we keep?\nsum(seq_table_nochim) / sum(seq_table)\n</code></pre> <p>As a final check of our progress, we\u2019ll look at the number of reads that made it through each step in the pipeline</p> <pre><code>get_n &lt;- function(x) sum(getUniques(x))\n\ntrack &lt;- cbind(out, sapply(dada_forward, get_n), sapply(merged_reads, get_n),\n               rowSums(seq_table), rowSums(seq_table_nochim))\n\ncolnames(track) &lt;- c('input', 'filtered', 'denoised', 'merged', 'tabled',\n                     'nonchim')\nrownames(track) &lt;- sample_names\nhead(track)\n</code></pre> <p>We kept the majority of our reads!</p>"},{"location":"usage/metabarcoding/#assign-taxonomy","title":"Assign Taxonomy","text":"<p>Now we assign taxonomy to our sequences using the SILVA database</p> <pre><code>taxa &lt;- assignTaxonomy(seq_table_nochim,\n                       'MiSeq_SOP/silva_nr_v128_train_set.fa.gz',\n                       multithread=TRUE)\ntaxa &lt;- addSpecies(taxa, 'MiSeq_SOP/silva_species_assignment_v128.fa.gz')\n</code></pre> <p>for inspecting the classification</p> <pre><code>taxa_print &lt;- taxa  # removing sequence rownames for display only\nrownames(taxa_print) &lt;- NULL\nhead(taxa_print)\n</code></pre>"},{"location":"usage/metabarcoding/#phylogenetic-tree","title":"Phylogenetic Tree","text":"<p>DADA2 is reference-free so we have to build the tree ourselves</p> <p>We first align our sequences</p> <pre><code>sequences &lt;- getSequences(seq_table)\nnames(sequences) &lt;- sequences  # this propagates to the tip labels of the tree\nalignment &lt;- AlignSeqs(DNAStringSet(sequences), anchor=NA)\n</code></pre> <p>Then we build a neighbour-joining tree then fit a maximum likelihood tree using the neighbour-joining tree as a starting point</p> <pre><code>phang_align &lt;- phyDat(as(alignment, 'matrix'), type='DNA')\ndm &lt;- dist.ml(phang_align)\ntreeNJ &lt;- NJ(dm)  # note, tip order != sequence order\nfit = pml(treeNJ, data=phang_align)\n\n## negative edges length changed to 0!\n\nfitGTR &lt;- update(fit, k=4, inv=0.2)\nfitGTR &lt;- optim.pml(fitGTR, model='GTR', optInv=TRUE, optGamma=TRUE,\n                    rearrangement = 'stochastic',\n                    control = pml.control(trace = 0))\ndetach('package:phangorn', unload=TRUE)\n</code></pre>"},{"location":"usage/metabarcoding/#phyloseq","title":"Phyloseq","text":"<p>First load the metadata</p> <pre><code>sample_data &lt;- read.table(\n    'https://hadrieng.github.io/tutorials/data/16S_metadata.txt',\n    header=TRUE, row.names=\"sample_name\")\n</code></pre> <p>We can now construct a phyloseq object from our output and newly created metadata</p> <pre><code>physeq &lt;- phyloseq(otu_table(seq_table_nochim, taxa_are_rows=FALSE),\n                   sample_data(sample_data),\n                   tax_table(taxa),\n                   phy_tree(fitGTR$tree))\n# remove mock sample\nphyseq &lt;- prune_samples(sample_names(physeq) != 'Mock', physeq)\nphyseq\n</code></pre> <p>Let's look at the alpha diversity of our samples</p> <pre><code>plot_richness(physeq, x='day', measures=c('Shannon', 'Fisher'), color='when') +\n    theme_minimal()\n</code></pre> <p>No obvious differences. Let's look at ordination methods (beta diversity)</p> <p>We can perform an MDS with euclidean distance (mathematically equivalent to a PCA)</p> <pre><code>ord &lt;- ordinate(physeq, 'MDS', 'euclidean')\nplot_ordination(physeq, ord, type='samples', color='when',\n                title='PCA of the samples from the MiSeq SOP') +\n    theme_minimal()\n</code></pre> <p>now with the Bray-Curtis distance</p> <pre><code>ord &lt;- ordinate(physeq, 'NMDS', 'bray')\nplot_ordination(physeq, ord, type='samples', color='when',\n                title='PCA of the samples from the MiSeq SOP') +\n    theme_minimal()\n</code></pre> <p>There we can see a clear difference between our samples.</p> <p>Let us take a look a the distribution of the most abundant families</p> <pre><code>top20 &lt;- names(sort(taxa_sums(physeq), decreasing=TRUE))[1:20]\nphyseq_top20 &lt;- transform_sample_counts(physeq, function(OTU) OTU/sum(OTU))\nphyseq_top20 &lt;- prune_taxa(top20, physeq_top20)\nplot_bar(physeq_top20, x='day', fill='Family') +\n    facet_wrap(~when, scales='free_x') +\n    theme_minimal()\n</code></pre> <p>We can place them in a tree</p> <pre><code>bacteroidetes &lt;- subset_taxa(physeq, Phylum %in% c('Bacteroidetes'))\nplot_tree(bacteroidetes, ladderize='left', size='abundance',\n          color='when', label.tips='Family')\n</code></pre>"}]}